{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "F2zogaJTQ_hS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# ReLU activation function\n",
        "def ReLU(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "# Derivative of ReLU\n",
        "def ReLU_derivative(x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "# Softmax activation function\n",
        "def Softmaxfxn(x):\n",
        "    exp_x = np.exp(x - np.max(x, axis=0, keepdims=True))  # Stabilize for numerical safety\n",
        "    return exp_x / np.sum(exp_x, axis=0, keepdims=True)\n",
        "\n",
        "# Cross-entropy loss function\n",
        "def cross_entropy_loss(y_true, y_pred):\n",
        "    # Add a small value (epsilon) to prevent log(0)\n",
        "    epsilon = 1e-12\n",
        "    y_pred = np.clip(y_pred, epsilon, 1. - epsilon)\n",
        "    return -np.mean(np.sum(y_true * np.log(y_pred), axis=0))\n",
        "\n",
        "\n",
        "# Gradient of the cross-entropy loss\n",
        "def loss_gradient(y_true, y_pred):\n",
        "    return y_pred - y_true\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Layer:\n",
        "    def __init__(self, input_size, num_neurons, activation_fxn):\n",
        "        # Xavier Initialization\n",
        "        limit = np.sqrt(6 / (input_size + num_neurons))\n",
        "        self.weights = np.random.uniform(-limit, limit, (num_neurons, input_size))\n",
        "        self.bias = np.zeros((num_neurons, 1))\n",
        "        self.activation_fxn = activation_fxn\n",
        "\n",
        "\n",
        "    def forward_propagation(self, inputs):\n",
        "        self.a_prev = inputs  # Store inputs for backpropagation\n",
        "        self.z = np.dot(self.weights, inputs) + self.bias  # Linear transformation\n",
        "        self.a = self.activation_fxn(self.z)  # Apply activation function\n",
        "        return self.a\n",
        "\n",
        "    def gradient_calc(self, dl_dy_pred, weights_next, z, a_prev):\n",
        "        \"\"\"\n",
        "    dl_dy_pred: Gradient of the loss with respect to the current layer's output\n",
        "    weights_next: Weights of the next layer (used for propagating gradients)\n",
        "    z: Pre-activation outputs of the current layer\n",
        "    a_prev: Activations from the previous layer (input to the current layer)\n",
        "     \"\"\"\n",
        "    # Compute gradient of loss with respect to the current layer's pre-activation output\n",
        "        self.dl_dz = dl_dy_pred * ReLU_derivative(z)  # Element-wise multiplication\n",
        "\n",
        "    # Compute gradients for the weights and biases\n",
        "        dl_dw = np.dot(self.dl_dz, a_prev.T)  # Gradient w.r.t. weights\n",
        "        dl_db = np.sum(self.dl_dz, axis=1, keepdims=True)  # Gradient w.r.t. biases\n",
        "\n",
        "    # Compute the gradient to pass to the previous layer\n",
        "        dl_da_prev = np.dot(self.weights.T, self.dl_dz)  # Gradient to pass to the previous layer\n",
        "\n",
        "        return dl_da_prev, dl_dw, dl_db\n"
      ],
      "metadata": {
        "id": "CHCFYTBnREjq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, layers):\n",
        "        self.layers = layers\n",
        "\n",
        "    def forward_pass(self, inputs):\n",
        "        for layer in self.layers:\n",
        "            inputs = layer.forward_propagation(inputs)\n",
        "        return inputs\n",
        "\n",
        "    def backward_pass(self, expected, actual, learning_rate):\n",
        "        # Compute the gradient of the loss with respect to the output\n",
        "        loss_grad = loss_gradient(expected, actual)\n",
        "\n",
        "        # Iterate through layers in reverse order (backpropagation)\n",
        "        for layer in reversed(self.layers):\n",
        "            # Call gradient_calc with the current layer's weights\n",
        "            loss_grad, dl_dw, dl_db = layer.gradient_calc(loss_grad, None, layer.z, layer.a_prev)\n",
        "\n",
        "            # Update the weights and biases using gradient descent\n",
        "            layer.weights -= dl_dw * learning_rate\n",
        "            layer.bias -= dl_db * learning_rate\n",
        "\n",
        "# Define network structure\n",
        "input_layer = Layer(784, 128, ReLU)  # Input layer\n",
        "hidden_layer_1 = Layer(128, 64, ReLU)  # Hidden layer 1\n",
        "hidden_layer_2 = Layer(64, 32, ReLU) #Hidden layer 2\n",
        "output_layer = Layer(32, 10, Softmaxfxn)  # Output layer\n",
        "\n",
        "nn = NeuralNetwork([input_layer, hidden_layer_1,hidden_layer_2, output_layer])\n",
        "\n",
        "# Training configuration\n",
        "epochs = 30\n",
        "learning_rate = 0.01\n",
        "batch_size = 8\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "df = pd.read_csv(\"/content/fashion-mnist_train.csv\")\n",
        "\n",
        "# Split dataset into training and testing subsets\n",
        "df_randomized = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Limit to the first 700 rows for testing and the next 1000-1700 rows for training\n",
        "test_df = df_randomized.head(1000)\n",
        "train_df = df_randomized.iloc[1000:2700]\n",
        "# Extract training data\n",
        "train_inputs = train_df.iloc[:, 1:].values  # Input data\n",
        "train_labels = train_df.iloc[:, 0].values  # Output labels\n",
        "\n",
        "# Extract testing data\n",
        "test_inputs = test_df.iloc[:, 1:].values  # Input data\n",
        "test_labels = test_df.iloc[:, 0].values  # Output labels\n",
        "\n",
        "# Normalize input data\n",
        "train_inputs = train_inputs / 255.0  # Normalize pixel values to [0, 1]\n",
        "test_inputs = test_inputs / 255.0  # Normalize pixel values to [0, 1]\n",
        "\n",
        "# One-hot encode the training labels\n",
        "num_classes = 10\n",
        "train_expected_outputs = np.zeros((len(train_labels), num_classes))\n",
        "for idx, label in enumerate(train_labels):\n",
        "    train_expected_outputs[idx, label] = 1\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    for i in range(0, len(train_inputs), batch_size):\n",
        "        # Load batch data\n",
        "        inputs = train_inputs[i:i+batch_size].T\n",
        "        expected_output = train_expected_outputs[i:i+batch_size].T\n",
        "\n",
        "        # Forward pass\n",
        "        predicted_output = nn.forward_pass(inputs)\n",
        "\n",
        "        # Backward pass\n",
        "        nn.backward_pass(expected_output, predicted_output, learning_rate)\n",
        "\n",
        "    # Calculate and print loss after each epoch\n",
        "    total_loss = cross_entropy_loss(train_expected_outputs.T, nn.forward_pass(train_inputs.T))\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {total_loss}\")\n",
        "\n",
        "# Testing phase\n",
        "test_predictions = nn.forward_pass(test_inputs.T)  # Forward pass\n",
        "predicted_labels = np.argmax(test_predictions, axis=0)  # Get the predicted labels\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(test_labels, predicted_labels)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Detailed classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(test_labels, predicted_labels))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ry3rGE8_RGcQ",
        "outputId": "22204727-3eff-4215-bae3-906a2f394ede"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.964883932741511\n",
            "Epoch 2, Loss: 0.8325418046557895\n",
            "Epoch 3, Loss: 0.6427897818387459\n",
            "Epoch 4, Loss: 0.5188777575403174\n",
            "Epoch 5, Loss: 0.4733389518215671\n",
            "Epoch 6, Loss: 0.6385068432074813\n",
            "Epoch 7, Loss: 0.41545173324338974\n",
            "Epoch 8, Loss: 0.39821655755390223\n",
            "Epoch 9, Loss: 0.42808307537021545\n",
            "Epoch 10, Loss: 0.4273338976128959\n",
            "Epoch 11, Loss: 0.37803815516957084\n",
            "Epoch 12, Loss: 0.44618504939083625\n",
            "Epoch 13, Loss: 0.3481539479613773\n",
            "Epoch 14, Loss: 0.37253431376597324\n",
            "Epoch 15, Loss: 0.3586415663530192\n",
            "Epoch 16, Loss: 0.28953277587511744\n",
            "Epoch 17, Loss: 0.2907849449070372\n",
            "Epoch 18, Loss: 0.3236368403589991\n",
            "Epoch 19, Loss: 0.27316994369489817\n",
            "Epoch 20, Loss: 0.2877658505670067\n",
            "Epoch 21, Loss: 0.5273772932678682\n",
            "Epoch 22, Loss: 0.26939174463097226\n",
            "Epoch 23, Loss: 0.28352827603133707\n",
            "Epoch 24, Loss: 0.22411763287727254\n",
            "Epoch 25, Loss: 0.39547332487571174\n",
            "Epoch 26, Loss: 0.2608466935481284\n",
            "Epoch 27, Loss: 0.2081244317052834\n",
            "Epoch 28, Loss: 0.28060624351684765\n",
            "Epoch 29, Loss: 0.21750997809443942\n",
            "Epoch 30, Loss: 0.21671967319731686\n",
            "Test Accuracy: 82.70%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.82      0.78        87\n",
            "           1       0.98      0.92      0.95       104\n",
            "           2       0.86      0.75      0.80       114\n",
            "           3       0.84      0.81      0.83       102\n",
            "           4       0.77      0.63      0.69        94\n",
            "           5       0.92      0.88      0.90       101\n",
            "           6       0.55      0.68      0.61       112\n",
            "           7       0.96      0.86      0.91        94\n",
            "           8       0.91      0.96      0.94        84\n",
            "           9       0.85      0.97      0.91       108\n",
            "\n",
            "    accuracy                           0.83      1000\n",
            "   macro avg       0.84      0.83      0.83      1000\n",
            "weighted avg       0.84      0.83      0.83      1000\n",
            "\n"
          ]
        }
      ]
    }
  ]
}