{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "J-Ru72h19vTD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# ReLU activation function\n",
        "def ReLU(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "# Derivative of ReLU\n",
        "def ReLU_derivative(x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "# Softmax activation function\n",
        "def Softmaxfxn(x):\n",
        "    exp_x = np.exp(x - np.max(x, axis=0, keepdims=True))  # Stabilize for numerical safety\n",
        "    return exp_x / np.sum(exp_x, axis=0, keepdims=True)\n",
        "\n",
        "# Cross-entropy loss function\n",
        "def cross_entropy_loss(y_true, y_pred):\n",
        "    # Add a small value (epsilon) to prevent log(0)\n",
        "    epsilon = 1e-12\n",
        "    y_pred = np.clip(y_pred, epsilon, 1. - epsilon)\n",
        "    return -np.mean(np.sum(y_true * np.log(y_pred), axis=0))\n",
        "\n",
        "\n",
        "# Gradient of the cross-entropy loss\n",
        "def loss_gradient(y_true, y_pred):\n",
        "    return y_pred - y_true\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4niONLC492p_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Layer:\n",
        "    def __init__(self, input_size, num_neurons, activation_fxn):\n",
        "        # Xavier Initialization\n",
        "        limit = np.sqrt(6 / (input_size + num_neurons))\n",
        "        self.weights = np.random.uniform(-limit, limit, (num_neurons, input_size))\n",
        "        self.bias = np.zeros((num_neurons, 1))\n",
        "        self.activation_fxn = activation_fxn\n",
        "\n",
        "\n",
        "    def forward_propagation(self, inputs):\n",
        "        self.a_prev = inputs  # Store inputs for backpropagation\n",
        "        self.z = np.dot(self.weights, inputs) + self.bias  # Linear transformation\n",
        "        self.a = self.activation_fxn(self.z)  # Apply activation function\n",
        "        return self.a\n",
        "\n",
        "    def gradient_calc(self, dl_dy_pred, weights_next, z, a_prev):\n",
        "        \"\"\"\n",
        "    dl_dy_pred: Gradient of the loss with respect to the current layer's output\n",
        "    weights_next: Weights of the next layer (used for propagating gradients)\n",
        "    z: Pre-activation outputs of the current layer\n",
        "    a_prev: Activations from the previous layer (input to the current layer)\n",
        "     \"\"\"\n",
        "    # Compute gradient of loss with respect to the current layer's pre-activation output\n",
        "        self.dl_dz = dl_dy_pred * ReLU_derivative(z)  # Element-wise multiplication\n",
        "\n",
        "    # Compute gradients for the weights and biases\n",
        "        dl_dw = np.dot(self.dl_dz, a_prev.T)  # Gradient w.r.t. weights\n",
        "        dl_db = np.sum(self.dl_dz, axis=1, keepdims=True)  # Gradient w.r.t. biases\n",
        "\n",
        "    # Compute the gradient to pass to the previous layer\n",
        "        dl_da_prev = np.dot(self.weights.T, self.dl_dz)  # Gradient to pass to the previous layer\n",
        "\n",
        "        return dl_da_prev, dl_dw, dl_db\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIs8UEgZ9z-1",
        "outputId": "666398df-f014-48f2-e559-48d22ce30df0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.45036300768693754\n",
            "Epoch 2, Loss: 0.24241360077388505\n",
            "Epoch 3, Loss: 0.1540164922160518\n",
            "Epoch 4, Loss: 0.1104867947188917\n",
            "Epoch 5, Loss: 0.09295409774616363\n",
            "Epoch 6, Loss: 0.06141307076347959\n",
            "Epoch 7, Loss: 0.04364381145980653\n",
            "Epoch 8, Loss: 0.02960921126815362\n",
            "Epoch 9, Loss: 0.023274046486618162\n",
            "Epoch 10, Loss: 0.01743067131578412\n",
            "Epoch 11, Loss: 0.014556065758653586\n",
            "Epoch 12, Loss: 0.012384396485430204\n",
            "Epoch 13, Loss: 0.010801084755738877\n",
            "Epoch 14, Loss: 0.009555266144449908\n",
            "Epoch 15, Loss: 0.006542609036788166\n",
            "Epoch 16, Loss: 0.004853919528918299\n",
            "Epoch 17, Loss: 0.004035728814946066\n",
            "Epoch 18, Loss: 0.0035015756565726914\n",
            "Epoch 19, Loss: 0.0030810671301502064\n",
            "Epoch 20, Loss: 0.002755353524674634\n",
            "Epoch 21, Loss: 0.002509073275066234\n",
            "Epoch 22, Loss: 0.0022867321389078705\n",
            "Epoch 23, Loss: 0.002115651530329816\n",
            "Epoch 24, Loss: 0.001961589524692625\n",
            "Epoch 25, Loss: 0.0018244347185871672\n",
            "Epoch 26, Loss: 0.001713407927200236\n",
            "Epoch 27, Loss: 0.0016056384054670535\n",
            "Epoch 28, Loss: 0.001520150301856875\n",
            "Epoch 29, Loss: 0.001437207494830472\n",
            "Epoch 30, Loss: 0.0013613981037763783\n",
            "Epoch 31, Loss: 0.001297693282014434\n",
            "Epoch 32, Loss: 0.0012369465673084548\n",
            "Epoch 33, Loss: 0.0011803852545008808\n",
            "Epoch 34, Loss: 0.001131478657499618\n",
            "Epoch 35, Loss: 0.0010834542967457716\n",
            "Epoch 36, Loss: 0.0010419870063054192\n",
            "Epoch 37, Loss: 0.001002364459195155\n",
            "Epoch 38, Loss: 0.0009643830557731673\n",
            "Epoch 39, Loss: 0.0009305400171928799\n",
            "Epoch 40, Loss: 0.0008983758095736867\n",
            "Epoch 41, Loss: 0.0008678570203943099\n",
            "Epoch 42, Loss: 0.0008401145129736575\n",
            "Epoch 43, Loss: 0.0008129720980107724\n",
            "Epoch 44, Loss: 0.0007886405853611397\n",
            "Epoch 45, Loss: 0.0007644442570002587\n",
            "Epoch 46, Loss: 0.0007428800088697283\n",
            "Epoch 47, Loss: 0.0007219050184756112\n",
            "Epoch 48, Loss: 0.0007012860550274967\n",
            "Epoch 49, Loss: 0.0006832877756067497\n",
            "Epoch 50, Loss: 0.000664929477670019\n",
            "Test Accuracy: 90.57%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.97      0.94        68\n",
            "           1       0.99      0.98      0.98        89\n",
            "           2       0.94      0.84      0.89        74\n",
            "           3       0.89      0.90      0.90        82\n",
            "           4       0.89      0.91      0.90        56\n",
            "           5       0.86      0.84      0.85        51\n",
            "           6       0.91      0.97      0.94        88\n",
            "           7       0.88      0.91      0.90        76\n",
            "           8       0.83      0.90      0.87        50\n",
            "           9       0.90      0.79      0.84        66\n",
            "\n",
            "    accuracy                           0.91       700\n",
            "   macro avg       0.90      0.90      0.90       700\n",
            "weighted avg       0.91      0.91      0.91       700\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, layers):\n",
        "        self.layers = layers\n",
        "\n",
        "    def forward_pass(self, inputs):\n",
        "        for layer in self.layers:\n",
        "            inputs = layer.forward_propagation(inputs)\n",
        "        return inputs\n",
        "\n",
        "    def backward_pass(self, expected, actual, learning_rate):\n",
        "        # Compute the gradient of the loss with respect to the output\n",
        "        loss_grad = loss_gradient(expected, actual)\n",
        "\n",
        "        # Iterate through layers in reverse order (backpropagation)\n",
        "        for layer in reversed(self.layers):\n",
        "            # Call gradient_calc with the current layer's weights\n",
        "            loss_grad, dl_dw, dl_db = layer.gradient_calc(loss_grad, None, layer.z, layer.a_prev)\n",
        "\n",
        "            # Update the weights and biases using gradient descent\n",
        "            layer.weights -= dl_dw * learning_rate\n",
        "            layer.bias -= dl_db * learning_rate\n",
        "\n",
        "# Define network structure\n",
        "input_layer = Layer(784, 128, ReLU)  # Input layer\n",
        "hidden_layer_1 = Layer(128, 64, ReLU)  # Hidden layer\n",
        "output_layer = Layer(64, 10, Softmaxfxn)  # Output layer\n",
        "\n",
        "nn = NeuralNetwork([input_layer, hidden_layer_1, output_layer])\n",
        "\n",
        "# Training configuration\n",
        "epochs = 50\n",
        "learning_rate = 0.01\n",
        "batch_size = 6\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "df = pd.read_csv(\"/content/train.csv\")\n",
        "\n",
        "# Split dataset into training and testing subsets\n",
        "df_randomized = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Limit to the first 700 rows for testing and the next 1000-1700 rows for training\n",
        "test_df = df_randomized.head(700)\n",
        "train_df = df_randomized.iloc[700:1700]\n",
        "# Extract training data\n",
        "train_inputs = train_df.iloc[:, 1:].values  # Input data\n",
        "train_labels = train_df.iloc[:, 0].values  # Output labels\n",
        "\n",
        "# Extract testing data\n",
        "test_inputs = test_df.iloc[:, 1:].values  # Input data\n",
        "test_labels = test_df.iloc[:, 0].values  # Output labels\n",
        "\n",
        "# Normalize input data\n",
        "train_inputs = train_inputs / 255.0  # Normalize pixel values to [0, 1]\n",
        "test_inputs = test_inputs / 255.0  # Normalize pixel values to [0, 1]\n",
        "\n",
        "# One-hot encode the training labels\n",
        "num_classes = 10\n",
        "train_expected_outputs = np.zeros((len(train_labels), num_classes))\n",
        "for idx, label in enumerate(train_labels):\n",
        "    train_expected_outputs[idx, label] = 1\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    for i in range(0, len(train_inputs), batch_size):\n",
        "        # Load batch data\n",
        "        inputs = train_inputs[i:i+batch_size].T\n",
        "        expected_output = train_expected_outputs[i:i+batch_size].T\n",
        "\n",
        "        # Forward pass\n",
        "        predicted_output = nn.forward_pass(inputs)\n",
        "\n",
        "        # Backward pass\n",
        "        nn.backward_pass(expected_output, predicted_output, learning_rate)\n",
        "\n",
        "    # Calculate and print loss after each epoch\n",
        "    total_loss = cross_entropy_loss(train_expected_outputs.T, nn.forward_pass(train_inputs.T))\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {total_loss}\")\n",
        "\n",
        "# Testing phase\n",
        "test_predictions = nn.forward_pass(test_inputs.T)  # Forward pass\n",
        "predicted_labels = np.argmax(test_predictions, axis=0)  # Get the predicted labels\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(test_labels, predicted_labels)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Detailed classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(test_labels, predicted_labels))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}